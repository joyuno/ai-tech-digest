"""arXiv ë…¼ë¬¸ ìˆ˜ì§‘ê¸° - Hugging Face Daily Papers (ì¸ê¸°ë„ ê¸°ë°˜)"""

import requests
from datetime import datetime, timedelta
from typing import List, Dict


class ArxivCollector:
    """Hugging Face Daily Papersì—ì„œ ì¸ê¸° AI/ML ë…¼ë¬¸ ìˆ˜ì§‘ (upvotes ê¸°ë°˜)

    Hugging Face Daily PapersëŠ” arXiv ë…¼ë¬¸ ì¤‘ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì¸ê¸° ìˆëŠ” ë…¼ë¬¸ì„
    upvotes ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì œê³µí•©ë‹ˆë‹¤.
    """

    DAILY_PAPERS_API = "https://huggingface.co/api/daily_papers"

    def __init__(self, config: dict):
        self.max_results = config.get("max_results", 5)
        self.timeout = config.get("timeout", 15)

    def collect(self) -> List[Dict]:
        """ì¸ê¸°ë„ ìˆœìœ¼ë¡œ ë…¼ë¬¸ ìˆ˜ì§‘ (upvotes ê¸°ë°˜)"""
        results = []

        try:
            print("  ğŸ“Š Hugging Face Daily Papersì—ì„œ ì¸ê¸° ë…¼ë¬¸ ìˆ˜ì§‘ ì¤‘...")

            response = requests.get(
                self.DAILY_PAPERS_API,
                timeout=self.timeout,
                headers={"Accept": "application/json"}
            )
            response.raise_for_status()
            papers = response.json()

            if not papers:
                print("  âš ï¸ Daily Papersì—ì„œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return []

            # upvotes ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆì§€ë§Œ í™•ì‹¤í•˜ê²Œ)
            papers_sorted = sorted(
                papers,
                key=lambda x: x.get("paper", {}).get("upvotes", 0),
                reverse=True
            )

            for paper_data in papers_sorted[:self.max_results]:
                paper = paper_data.get("paper", {})

                # arXiv IDì—ì„œ URL ìƒì„±
                arxiv_id = paper.get("id", "")
                arxiv_url = f"https://arxiv.org/abs/{arxiv_id}" if arxiv_id else ""

                # ìš”ì•½ ì •ë¦¬
                summary = paper.get("summary", "")
                if summary:
                    summary = summary.replace('\n', ' ').strip()[:500]

                # upvotes ìˆ˜
                upvotes = paper.get("upvotes", 0)

                results.append({
                    "title": paper.get("title", "").strip(),
                    "url": arxiv_url,
                    "summary": summary,
                    "authors": paper.get("authors", [])[:3],
                    "published": paper.get("publishedAt", ""),
                    "upvotes": upvotes,
                    "source": "arxiv",
                })

            print(f"  âœ“ {len(results)}ê°œ ì¸ê¸° ë…¼ë¬¸ ìˆ˜ì§‘ (upvotes ê¸°ì¤€)")

        except requests.exceptions.RequestException as e:
            print(f"  âš ï¸ API ìš”ì²­ ì‹¤íŒ¨: {e}")
            return self._fallback_collect()
        except Exception as e:
            print(f"  âš ï¸ arXiv ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

        return results

    def _fallback_collect(self) -> List[Dict]:
        """API ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ arXiv ê²€ìƒ‰ìœ¼ë¡œ í´ë°±"""
        try:
            import arxiv
            from datetime import timezone

            print("  ğŸ”„ ê¸°ë³¸ arXiv ê²€ìƒ‰ìœ¼ë¡œ í´ë°±...")

            query = "cat:cs.AI OR cat:cs.LG OR cat:cs.CL"
            search = arxiv.Search(
                query=query,
                max_results=self.max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending,
            )

            results = []
            for paper in search.results():
                summary = paper.summary.replace('\n', ' ').strip()[:500]
                results.append({
                    "title": paper.title.strip(),
                    "url": paper.entry_id,
                    "summary": summary,
                    "authors": [a.name for a in paper.authors[:3]],
                    "published": paper.published.isoformat(),
                    "upvotes": 0,
                    "source": "arxiv",
                })

            return results
        except Exception as e:
            print(f"  âš ï¸ í´ë°±ë„ ì‹¤íŒ¨: {e}")
            return []


if __name__ == "__main__":
    """í…ŒìŠ¤íŠ¸ ì½”ë“œ"""
    print("=== arXiv Collector í…ŒìŠ¤íŠ¸ (ì¸ê¸°ë„ ê¸°ë°˜) ===\n")

    config = {"max_results": 5}
    collector = ArxivCollector(config)
    papers = collector.collect()

    print(f"\nì´ {len(papers)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘\n")
    print("-" * 80)

    for i, paper in enumerate(papers, 1):
        upvotes = paper.get('upvotes', 0)
        print(f"\n[{i}] ğŸ”¥ {upvotes} upvotes")
        print(f"ì œëª©: {paper['title']}")
        print(f"URL: {paper['url']}")
        print(f"ì €ì: {paper.get('authors', [])}")
        print(f"ìš”ì•½: {paper['summary'][:100]}...")
        print("-" * 80)
