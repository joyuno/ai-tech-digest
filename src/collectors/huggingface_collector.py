"""Hugging Face ìˆ˜ì§‘ê¸° - Daily Papers (ì¸ê¸°ë„ ê¸°ë°˜)"""

import requests
from typing import List, Dict
import logging

logger = logging.getLogger(__name__)


class HuggingFaceCollector:
    """Hugging Face Daily Papersì—ì„œ ì¸ê¸° ë…¼ë¬¸ ìˆ˜ì§‘ (upvotes ê¸°ë°˜)

    arXiv collectorì™€ ë‹¤ë¥¸ ë‚ ì§œ ë²”ìœ„ì˜ ë…¼ë¬¸ì„ ê°€ì ¸ì˜¤ê±°ë‚˜,
    ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ì˜ ì¸ê¸° ë…¼ë¬¸ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """

    # Hugging Face Papers API (ìµœê·¼ ì¸ê¸° ë…¼ë¬¸)
    PAPERS_API = "https://huggingface.co/api/papers"

    def __init__(self, config: dict):
        self.max_results = config.get("max_results", 3)
        self.timeout = config.get("timeout", 15)

    def collect(self) -> List[Dict]:
        """ì¸ê¸°ë„ ìˆœìœ¼ë¡œ ë…¼ë¬¸ ìˆ˜ì§‘ (upvotes ê¸°ë°˜)"""
        results = []

        try:
            print("  ğŸ“Š Hugging Face Papersì—ì„œ ì¸ê¸° ë…¼ë¬¸ ìˆ˜ì§‘ ì¤‘...")

            # Papers APIì—ì„œ ìµœê·¼ ì¸ê¸° ë…¼ë¬¸ ì¡°íšŒ
            response = requests.get(
                self.PAPERS_API,
                params={"limit": self.max_results * 2},  # ì—¬ìœ ë¶„
                timeout=self.timeout,
                headers={"Accept": "application/json"}
            )
            response.raise_for_status()
            papers = response.json()

            if not papers:
                print("  âš ï¸ Papersì—ì„œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return self._fallback_rss()

            # upvotes ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            papers_sorted = sorted(
                papers,
                key=lambda x: x.get("upvotes", 0),
                reverse=True
            )

            for paper in papers_sorted[:self.max_results]:
                # arXiv IDì—ì„œ URL ìƒì„±
                paper_id = paper.get("id", "")
                arxiv_url = f"https://arxiv.org/abs/{paper_id}" if paper_id else ""
                hf_url = f"https://huggingface.co/papers/{paper_id}" if paper_id else ""

                # ìš”ì•½ ì •ë¦¬
                summary = paper.get("summary", "")
                if summary:
                    summary = summary.replace('\n', ' ').strip()[:500]

                # upvotes ìˆ˜
                upvotes = paper.get("upvotes", 0)

                # ì €ì ì •ë³´
                authors = paper.get("authors", [])
                if isinstance(authors, list) and authors:
                    if isinstance(authors[0], dict):
                        author_names = [a.get("name", "") for a in authors[:3]]
                    else:
                        author_names = authors[:3]
                else:
                    author_names = []

                results.append({
                    "title": paper.get("title", "").strip(),
                    "url": hf_url or arxiv_url,
                    "summary": summary,
                    "author": ", ".join(author_names) if author_names else "Hugging Face",
                    "published": paper.get("publishedAt", ""),
                    "upvotes": upvotes,
                    "source": "huggingface",
                })

            print(f"  âœ“ {len(results)}ê°œ ì¸ê¸° ë…¼ë¬¸ ìˆ˜ì§‘ (upvotes ê¸°ì¤€)")

        except requests.exceptions.RequestException as e:
            print(f"  âš ï¸ API ìš”ì²­ ì‹¤íŒ¨: {e}")
            return self._fallback_rss()
        except Exception as e:
            print(f"  âš ï¸ HuggingFace ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            logger.error(f"Error: {e}")
            return []

        return results

    def _fallback_rss(self) -> List[Dict]:
        """API ì‹¤íŒ¨ ì‹œ ë¸”ë¡œê·¸ RSSë¡œ í´ë°±"""
        import feedparser

        print("  ğŸ”„ ë¸”ë¡œê·¸ RSSë¡œ í´ë°±...")

        try:
            feed = feedparser.parse("https://huggingface.co/blog/feed.xml")

            if not feed.entries:
                return []

            results = []
            for entry in feed.entries[:self.max_results]:
                summary = entry.get("summary", "").strip()[:500]
                results.append({
                    "title": entry.get("title", "Untitled"),
                    "url": entry.get("link", ""),
                    "summary": summary,
                    "author": entry.get("author", "Hugging Face"),
                    "published": entry.get("published", ""),
                    "upvotes": 0,
                    "source": "huggingface",
                })

            return results
        except Exception as e:
            print(f"  âš ï¸ RSS í´ë°±ë„ ì‹¤íŒ¨: {e}")
            return []


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    print("=" * 80)
    print("Hugging Face Collector í…ŒìŠ¤íŠ¸ (ì¸ê¸°ë„ ê¸°ë°˜)")
    print("=" * 80)

    config = {"max_results": 5}
    collector = HuggingFaceCollector(config)

    try:
        articles = collector.collect()
        print(f"\nìˆ˜ì§‘ëœ ë…¼ë¬¸: {len(articles)}ê°œ\n")

        for i, article in enumerate(articles, 1):
            upvotes = article.get('upvotes', 0)
            print(f"{i}. ğŸ”¥ {upvotes} upvotes")
            print(f"   ì œëª©: {article['title']}")
            print(f"   URL: {article['url']}")
            print(f"   ì €ì: {article['author']}")
            print(f"   ìš”ì•½: {article['summary'][:80]}...")
            print()

    except Exception as e:
        print(f"ERROR: {e}")
        import traceback
        traceback.print_exc()
